{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99546,"databundleVersionId":11895149,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1 Importing Library and Loading Dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, FunctionTransformer, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom category_encoders import TargetEncoder\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom scipy.stats import randint, uniform\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv\") \ntest_data = pd.read_csv(\"/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2 Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Duplicate Rows","metadata":{}},{"cell_type":"code","source":"total_duplicates = df.duplicated().sum()\nprint(f\"Total duplicate rows: {total_duplicates}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 Single value columns","metadata":{}},{"cell_type":"code","source":"single_value_columns = [col for col in df.columns if df[col].nunique(dropna=False) == 1]\nprint(\"Number of Single Value Columns:\", len(single_value_columns))\n\n\nif single_value_columns:\n    single_value_info = []\n    for col in single_value_columns:\n        constant_value = df[col].iloc[0]\n        single_value_info.append({\n            'Column': col,\n            'Constant Value': constant_value,\n            'Data Type': str(df[col].dtype)\n        })\n    \n    single_value_df = pd.DataFrame(single_value_info)\n    print(\"\\nSingle Value Columns Table:\")\n    print(\"=\" * 50)\n    print(single_value_df.to_string(index=False))\nelse:\n    print(\"\\nNo single value columns found.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3 Missing values","metadata":{}},{"cell_type":"code","source":"# Replace values with np.nan\nvalues_to_replace = ['not available in demo dataset', '(not set)', '(none)']\n\ndf['geoNetwork.continent'] = df['geoNetwork.continent'].replace(values_to_replace, np.nan)\ndf['trafficSource.medium'] = df['trafficSource.medium'].replace(values_to_replace, np.nan)\n\ntest_data['geoNetwork.continent'] = test_data['geoNetwork.continent'].replace(values_to_replace, np.nan)\ntest_data['trafficSource.medium'] = test_data['trafficSource.medium'].replace(values_to_replace, np.nan)\n\nthreshold = 0.6\nmissing_percentages = df.isnull().mean() * 100\n\nhigh_missing_cols = missing_percentages[missing_percentages > (threshold * 100)]\n\nmissing_table = pd.DataFrame({\n    'Column': high_missing_cols.index,\n    'Missing_Percentage': high_missing_cols.values\n}).round(2)\n\nprint(\"Columns with >60% missing values:\")\nprint(\"\\n\" + \"=\"*80)\nprint(f\"{'Column Name':<60} {'Missing %':<10}\")\nprint(\"=\"*80)\nfor col, pct in high_missing_cols.items():\n    print(f\"{col:<60} {pct:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.4 High Cardinal Values","metadata":{}},{"cell_type":"code","source":"cardinality_info = []\nhigh_cardinality_cols = []\n\nfor col in df.select_dtypes(include=['object']).columns:\n    unique_count = df[col].nunique(dropna=False)\n    \n    if unique_count > 200:\n        cardinality_info.append({\n            'Column_Name': col,\n            'Unique_Count': unique_count,\n            'Total_Rows': len(df),\n            'Null_Count': df[col].isnull().sum(),\n            'Data_Type': str(df[col].dtype)\n        })\n        high_cardinality_cols.append(col)\n\nif cardinality_info:\n    cardinality_df = pd.DataFrame(cardinality_info)\n    print(\"High Cardinality Columns (>200 unique values):\")\n    print(\"=\" * 80)\n    print(cardinality_df.to_string(index=False))\n    print(\"=\" * 80)\n    print(f\"Total high cardinality columns found: {len(cardinality_info)}\")\nelse:\n    print(\"No columns with >200 unique values found.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.5 Univariate Analysis ","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe(include=[\"object\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe(include=[\"bool\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### PurchaseValue","metadata":{}},{"cell_type":"code","source":"plt.hist(df[\"purchaseValue\"], bins=10,ec=\"black\", log=True)\nplt.xlabel(\"Purchase Value\")\nplt.ylabel(\"frequency\")\nplt.title(\"Histogram of PurchaseValue\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Totals Bounces\n\n- when totals.bounces is 1, purchaseValue drops to 0","metadata":{}},{"cell_type":"code","source":"print(df[df['totals.bounces'] == 1]['purchaseValue'].value_counts(dropna=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### new_visits","metadata":{}},{"cell_type":"code","source":"df['new_visits'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df[df['new_visits'] == 1]['purchaseValue'].value_counts(dropna=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## userId, sessionId\n- very high variability in the userId, sessinoId","metadata":{}},{"cell_type":"code","source":"plt.hist(df[\"userId\"], bins=10,ec=\"black\")\nplt.xlabel(\"ID\")\nplt.ylabel(\"userId\")\nplt.title(\"Histogram of userId\")\nplt.show()\n\nplt.hist(df[\"sessionId\"], bins=10,ec=\"black\")\nplt.xlabel(\"sessionId\")\nplt.ylabel(\"frequency\")\nplt.title(\"Histogram of sessionId\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Total rows: {len(df)}\")\nid_cols = ['userId','sessionId']\nfor id_c in id_cols:\n    print(f\"Unique values in column: {df[id_c].nunique(dropna=False)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Browser","metadata":{}},{"cell_type":"code","source":"df['browser'].value_counts(normalize=True,dropna=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GeoCluster","metadata":{}},{"cell_type":"code","source":"df['geoCluster'].value_counts(normalize=True,dropna=False)\n\nplt.hist(df[\"geoCluster\"], bins=9,ec=\"black\")\nplt.xlabel(\"GeoCluster\")\nplt.ylabel(\"frequency\")\nplt.title(\"Histogram of GeoCluster\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### geoNetwork.networkDomain","metadata":{}},{"cell_type":"code","source":"plt.hist(df[\"geoNetwork.networkDomain\"], bins=5,ec=\"black\")\nplt.xlabel(\"geoNetwork.networkDomain\")\nplt.ylabel(\"frequency\")\nplt.title(\"Histogram of geoNetwork.networkDomain\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### gcIdPresent","metadata":{}},{"cell_type":"code","source":"df['gclIdPresent'].value_counts(dropna=False, normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### sessionNumber\n\n- most of the sessions are shorter","metadata":{}},{"cell_type":"code","source":"plt.hist(df[\"sessionNumber\"], bins=100,ec=\"black\")\nplt.xlabel(\"sessionNumber\")\nplt.ylabel(\"frequency\")\nplt.title(\"Histogram of sessionNumber\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### trafficSource","metadata":{}},{"cell_type":"code","source":"df['trafficSource'].value_counts(dropna=False, normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### os","metadata":{}},{"cell_type":"code","source":"df['os'].value_counts(dropna=False, normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### trafficSource.medium","metadata":{}},{"cell_type":"code","source":"df['trafficSource.medium'].value_counts(dropna=False, normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Location","metadata":{}},{"cell_type":"code","source":"df['locationCountry'].value_counts(dropna=False, normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['geoNetwork.continent'].value_counts(dropna=False, normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['geoNetwork.subContinent'].value_counts(dropna=False, normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Page Views","metadata":{}},{"cell_type":"code","source":"df['pageViews'].value_counts(normalize=True, dropna=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Device Types","metadata":{}},{"cell_type":"code","source":"plt.hist(df[\"deviceType\"], bins=9,ec=\"black\")\nplt.xlabel(\"deviceType\")\nplt.ylabel(\"frequency\")\nplt.title(\"Histogram of deviceType\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### User Channel","metadata":{}},{"cell_type":"code","source":"df['userChannel'].value_counts(dropna=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### totalHits","metadata":{}},{"cell_type":"code","source":"df['totalHits'].value_counts(normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### date","metadata":{}},{"cell_type":"code","source":"df['date'].value_counts(dropna=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax1 = sns.displot(data=df, x=\"date\", kde=True, bins=100, color=\"red\", \n            facecolor=\"#FF0000\", height=5, aspect=3.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### userChannel","metadata":{}},{"cell_type":"code","source":"df['userChannel'].value_counts(normalize=True,dropna=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### totalHits","metadata":{}},{"cell_type":"code","source":"df['totalHits'].value_counts(normalize=True,dropna=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### device.isMobile","metadata":{}},{"cell_type":"code","source":"df['device.isMobile'].value_counts(normalize=True,dropna=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### sessionStart","metadata":{}},{"cell_type":"code","source":"df['sessionStart_dt'] = pd.to_datetime(df['sessionStart'], unit='s')\ntest_data['sessionStart_dt'] = pd.to_datetime(test_data['sessionStart'], unit='s')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# purchases by sessionStart \ndf['sessionStart_dt'] = pd.to_datetime(df['sessionStart'], unit='s')\nax1 = sns.displot(data=df, x=\"sessionStart_dt\", kde=True, bins=100, color=\"red\", \n            facecolor=\"#FF0000\", height=5, aspect=3.5)\ndf.drop(['sessionStart_dt'], axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.6 Bivariate Analysis","metadata":{}},{"cell_type":"code","source":"num_cols = df.select_dtypes(exclude=['object']).columns.tolist()\n\ncorr_matrix = df[num_cols].corr()\n\n# Plot heatmap\nplt.figure(figsize=(14, 10))\nsns.heatmap(corr_matrix, cmap='RdBu', annot=True, fmt=\".2f\", center=0,\n            linewidths=0.5, linecolor='white', cbar_kws={\"shrink\": 0.75})\n\nplt.title(\"Correlation Matrix  for train (Red to Blue)\", fontsize=16)\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Group by year and continent, summing purchaseValue\ndf['sessionStart_dt'] = pd.to_datetime(df['sessionStart'], unit='s')\n\ndf['year'] = df['sessionStart_dt'].dt.year\ndf['month'] = df['sessionStart_dt'].dt.month\ndf['day'] = df['sessionStart_dt'].dt.day\ndf['hour'] = df['sessionStart_dt'].dt.hour\n\nyearly_hits = df.groupby(['year','geoNetwork.continent'])['purchaseValue'].sum()\n\nyearly_hits.unstack().plot(\n    kind='bar',\n    stacked=True,\n    colormap='Spectral',\n    figsize=(13, 5),\n    grid=False,\n    alpha=0.9\n)\n\nplt.title('Purchase Value over Continent')\nplt.xlabel('Year')\nplt.ylabel('Purchase Value')\nplt.legend(title='Continent', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\nna_df = df[df['geoNetwork.continent'] == 'Americas']\n\nvalue_by_year = na_df.groupby(['year', 'geoNetwork.subContinent'])['purchaseValue'].sum()\n\nvalue_by_year.unstack().plot(\n    kind='bar',\n    stacked=True,\n    colormap='Spectral',\n    figsize=(13, 5),\n    grid=False,\n    alpha=0.9\n)\n\nplt.title('Purchase Value over American Continent')\nplt.xlabel('Year')\nplt.ylabel('Purchase Value')\nplt.legend(title='Continent', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\nax1 = sns.displot(data=df, x=\"sessionStart_dt\", weights=\"purchaseValue\", kde=True, bins=100, \n                  color=\"red\", facecolor=\"#FF0000\", height=5, aspect=3.5)\nax1.set_axis_labels(x_var=\"Session Start\", y_var=\"purchaseValue\")\n\nax1 = sns.displot(data=df, x=\"hour\", weights=\"purchaseValue\", kde=True, bins=100, \n                  color=\"red\", facecolor=\"#FF0000\", height=5, aspect=3.5)\nax1.set_axis_labels(x_var=\"Hour\", y_var=\"purchaseValue\")\n\n\ndf.drop(['sessionStart_dt', 'year', 'month', 'day', 'hour'], axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grouped = df.groupby(\"pageViews\")[\"purchaseValue\"].mean().reset_index()\n\nsns.lineplot(data=grouped, x=\"pageViews\", y=\"purchaseValue\", marker=\"o\")\nplt.title(\"Average Purchase Value vs Page Views\")\nplt.xlabel(\"Page Views\")\nplt.ylabel(\"Average Purchase Value\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grouped = df.groupby(\"sessionNumber\")[\"purchaseValue\"].mean().reset_index()\n\nsns.lineplot(data=grouped, x=\"sessionNumber\", y=\"purchaseValue\", marker=\"o\")\nplt.title(\"Average Purchase Value vs sessionNumber\")\nplt.xlabel(\"Session Number\")\nplt.ylabel(\"Average Purchase Value\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['new_visits_mod'] = df['new_visits'].fillna(0)\n\nsns.boxplot(data=df, x=\"new_visits_mod\", y=\"purchaseValue\")\nplt.title(\"Purchase Value by New vs Returning Visitors\")\nplt.xlabel(\"New Visitor (1 = Yes, 0 = No)\")\nplt.ylabel(\"Purchase Value\")\nplt.show()\n\n\nsns.relplot(\n    data=df, x=\"pageViews\", y=\"purchaseValue\",\n    col=\"new_visits_mod\", kind=\"scatter\", alpha=0.6\n)\nplt.suptitle(\"Page Views vs Purchase Value, by Visitor Type\", y=1.05)\nplt.show()\n\ndf.drop(['new_visits_mod'], inplace=True, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3 Feature Engineering","metadata":{}},{"cell_type":"code","source":"values_to_replace = ['not available in demo dataset', '(not set)']\n\ndf['geoNetwork.city'] = df['geoNetwork.city'].replace(values_to_replace, np.nan)\ndf['geoNetwork.metro'] = df['geoNetwork.metro'].replace(values_to_replace, np.nan)\n\ntest_data['geoNetwork.city'] = test_data['geoNetwork.city'].replace(values_to_replace, np.nan)\ntest_data['geoNetwork.metro'] = test_data['geoNetwork.metro'].replace(values_to_replace, np.nan)\n\n\n## Combining countries\ndef combine_countries(df):\n    df = df.copy()\n    df['continent_country'] = df['geoNetwork.continent'] + ' - ' + df['geoNetwork.subContinent'] + '-' + df['locationCountry']\n    return df.drop(['geoNetwork.continent', 'geoNetwork.subContinent', 'locationCountry'],axis=1)\ndf = combine_countries(df)\ntest_data = combine_countries(test_data)\n\n\n## sessionStart\ndef engineer_datetime_features(X, datetime_col):\n    #Extract the year,month,day,day of week, datetime and hour from Creation time stamp.\n    X['Year'] = pd.to_datetime(X[datetime_col], unit='s').dt.year\n    X['Month'] = pd.to_datetime(X[datetime_col], unit='s').dt.month\n    X['Day'] = pd.to_datetime(X[datetime_col], unit='s').dt.day\n    X['DayOfWeek'] = pd.to_datetime(X[datetime_col], unit='s').dt.strftime(\"%A\")\n    X[\"Hour\"]=pd.to_datetime(X[datetime_col],unit=\"s\").dt.hour\n    return X\n\ndf = engineer_datetime_features(df, 'sessionStart')\ntest_data = engineer_datetime_features(test_data, 'sessionStart')\n\n\nthreshold = 0.6\nhigh_missing_cols = df.columns[df.isnull().mean() > threshold]\ndf = df.loc[:, df.isnull().mean() <= threshold]\ntest_data = test_data.loc[:, test_data.isnull().mean() <= threshold]\n\n\n# Drop single value columns\nsingle_value_columns = [col for col in df.columns if df[col].nunique(dropna=False) == 1]\ndf.drop(single_value_columns, axis=1, inplace=True)\ntest_data.drop(single_value_columns, axis=1, inplace=True)\n\n\nhigh_cardinality_cols = []\nfor col in df.select_dtypes(include=['object', 'category']).columns:\n    unique_count = df[col].nunique(dropna=False)\n    if unique_count>200:\n        high_cardinality_cols.append(col)\ndf.drop(high_cardinality_cols, axis=1, inplace=True)\ntest_data.drop(high_cardinality_cols, axis=1, inplace=True)\n\n## Drop Id column\n# id_cols = ['userId','sessionId']\nid_cols = ['sessionId']\ndf.drop(id_cols, axis=1, inplace=True)\ntest_data.drop(id_cols, axis=1, inplace=True)\n\ndf['userId'] = df['userId'].astype('object')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3  Preprocessing","metadata":{}},{"cell_type":"code","source":"\nX = df.drop('purchaseValue', axis=1)\ny = df['purchaseValue']\n\nnumeric_columns = X.select_dtypes(include=['number']).columns\ncategorical_columns = X.select_dtypes(include=['object', 'category']).columns\n\n\nimputation_preprocessor = ColumnTransformer([\n    ('num_imputer', SimpleImputer(strategy='mean'), numeric_columns),\n    ('cat_imputer', SimpleImputer(strategy='most_frequent'), categorical_columns)\n])\n\npreprocessor = Pipeline([\n    ('imputer', imputation_preprocessor),\n    ('classifier', TargetEncoder()),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_binned = pd.cut(y, bins=7, labels=False)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, \n    test_size=0.1, \n    stratify=y_binned,\n    random_state=42\n)\n\nX_train = preprocessor.fit_transform(X_train,y_train)\n\nX_val = preprocessor.transform(X_val)\n\ntest_data = preprocessor.transform(test_data)\n\nprint(\"Pipeline created and fitted successfully.\")\nprint(\"Processed training data shape:\", X.shape)\nprint(\"Processed validation data shape:\", X_val.shape)\nprint(\"Processed test data shape:\", test_data.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4 Model Training","metadata":{}},{"cell_type":"code","source":"models = {\n    \"XGBoost Regressor\": XGBRegressor(random_state=42,verbose=-1),\n    \"Lasso Regression\": Lasso(alpha=0.2),\n    \"Ridge Regression\": Ridge(alpha=0.2),\n    \"Light Gradient Boosting Machine\": LGBMRegressor(verbose=-1),\n    \"Linear Regression\": LinearRegression(),\n    \"Random Forest\": RandomForestRegressor(),\n}\nresults = []\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    \n    rmse = np.sqrt(mean_squared_error(y_val, preds))\n    mae = mean_absolute_error(y_val, preds)\n    r2 = r2_score(y_val, preds)\n    \n    results.append((model, rmse, mae, r2))\n\nresults_df = pd.DataFrame(results, columns=['Model', 'RMSE', 'MAE', 'RÂ² Score'])\nresults_df = results_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\n\ndisplay(results_df)\n\nbest_model = results_df.loc[0, 'Model']\nprint(f\"Best performing model: {best_model}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5 Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# model =  LGBMRegressor(random_state=42, n_jobs=-1,verbose=-1,)\n# params = {\n#     \"n_estimators\": [150, 200, 300],\n#     \"max_depth\": [None, 5, 10],\n#     \"num_leaves\": [31, 50, 70],\n#     \"min_child_samples\": [5, 10, 20]\n# }\n\n# search = RandomizedSearchCV(\n#     estimator=model,\n#     param_distributions=params,\n#     n_iter=20,\n#     cv=3,\n#     scoring='neg_root_mean_squared_error',\n#     n_jobs=-1,\n#     verbose=-1,\n#     random_state=42\n# )\n# search.fit(X_train, y_train)\n# best_model = search.best_estimator_\n# print(\"Best Parameters:\", search.best_params_)\n\n# preds = best_model.predict(X_val)\n# r2 = r2_score(y_val, preds)\n\n\n# print(\"r2 score \", r2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Best Parameters: {'num_leaves': 50, 'n_estimators': 300, 'min_child_samples': 5, 'max_depth': None}\n- r2 score  0.3866706709560749","metadata":{}},{"cell_type":"code","source":"# model = RandomForestRegressor(random_state=42, n_jobs=-1)\n# params = {\n#     \"n_estimators\": [120, 300, 500],\n#     \"max_depth\": [None, 5, 10],\n#     \"min_samples_split\": [2,4,5],\n#     \"min_samples_leaf\": [1, 2,4,5]\n# }\n\n# search = RandomizedSearchCV(\n#     estimator=model,\n#     param_distributions=params,\n#     n_iter=20,\n#     cv=3,\n#     scoring='neg_root_mean_squared_error',\n#     n_jobs=-1,\n#     verbose=-1,\n#     random_state=42\n# )\n# search.fit(X_train, y_train)\n# best_model = search.best_estimator_\n# print(\"Best Parameters:\", search.best_params_)\n\n# preds = best_model.predict(X_val)\n# r2 = r2_score(y_val, preds)\n\n\n# print(\"r2 score \", r2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Best Parameters: {'n_estimators': 120, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None}\n- r2 score 0.3982068833082706\n\n","metadata":{}},{"cell_type":"code","source":"# model = XGBRegressor(random_state=42, verbosity=0, n_jobs=-1)\n# params = {\n#     'n_estimators': [500, 400, 300],\n#     'max_depth': [3, 5, 7, 11],\n#     'learning_rate': [0.01, 0.1, 0.2, 0.5],\n#     'subsample': [0.6, 0.8, 1.0],\n#     'colsample_bytree': [0.6, 0.8, 1.0,0.6]\n# }\n\n# search = RandomizedSearchCV(\n#     estimator=model,\n#     param_distributions=params,\n#     n_iter=30,\n#     cv=3,\n#     scoring='neg_root_mean_squared_error',\n#     n_jobs=-1,\n#     verbose=-1,\n#     random_state=42\n# )\n# search.fit(X_train, y_train)\n# best_model = search.best_estimator_\n# print(\"Best Parameters:\", search.best_params_)\n\n# preds = best_model.predict(X_val)\n# r2 = r2_score(y_val, preds)\n\n\n# print(\"r2 score \", r2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Best Parameters: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n\n- r2 score  0.40339698735560314","metadata":{}},{"cell_type":"markdown","source":"## 6 Best Model after tuning\n## - XGboost\n## - parameters  {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n","metadata":{}},{"cell_type":"code","source":"# parameters = {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.5, 'colsample_bytree': 0.6}\nbest_model =  XGBRegressor(max_depth=15, max_leaves=None,min_child_weight=None,missing=np.nan, monotone_constraints=None,multi_strategy=None,n_estimators=100,n_jobs=-1)\nbest_model.fit(X_train, y_train)\n\npreds = best_model.predict(X_val)\n\nrmse = np.sqrt(mean_squared_error(y_val, preds))\nmae = mean_absolute_error(y_val, preds)\nr2 = r2_score(y_val, preds)\n\nprint('r2 score', r2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred=best_model.predict(test_data)\n\nsubmission = pd.DataFrame({\"id\": range(0,test_data.shape[0]), \"purchaseValue\": y_pred})\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}